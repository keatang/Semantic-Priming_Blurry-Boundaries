## Semantic Priming Effects During Naturalistic Reading: Multiple Semantic-Primers or Just One?
#### Abstract
This study attempts to investigate the apparent blurry boundaries of the semantic priming effects during naturalistic reading. Data was collected on children between the ages of seven and nine (n = 278) on decoding errors made during a naturalistic passage reading task. The main goal of this study is to use Natural Language Processing (NLP) to determine if words could potentially have multiple seman-tic-primers or if words are only limited to just one semantic-primer. Semantic-similarity scores were calculated for each content word within each passage using one of two methods: (a) calculating semantic-similarity scores between target words and the preceding content word (paired semantic-similarity score), and (b) calculating semantic-similarity scores between target words and all preceding content words within the same sentence (coherence semantic-similarity score). Multivariate regression models were conducted to determine whether either of the two variables predict word decod-ing errors after controlling for word length and word frequency. Additionally, the amount of unique variance explained by each of the two variables was also computed. Both variables were predic-tive of word reading errors after controlling for word frequency and word length. However, coherence semantic-similarity scores seemed to explain more unique variance when compared to paired semantic-similarity scores. These results suggest that there may be multiple semantic-primers for each target word, and that there may be a blurry boundary to semantic-priming effects

#### Methods
##### Data (Not Included in Repository)
Data was collected across 278 participants, across two different samples who read different passages. Sample One had 142 partici-pants who read an experimenter-created passage, "Toads''. All par-ticipants were between the ages of seven to nine, of varying levels of reading ability. Sample Two had 136 participants, who were counterbalanced to have read two out of four possible passages ("The Surprise", "The Mouse in the House", "Air", and "The Brain and Five Senses"). These passages came from the Qualitative Reading Inventory [5] which is an assessment of students’ skills in reading from elementary to high school. All participants were between the ages of seven to nine, of varying levels of reading ability. For both samples, a master’s student listened to recordings of partici-pants during the oral reading task, and determined when and for which word a miscue error was made. 
A miscue was determined to be when a child mispronounced a word during a passage reading task. A zero meant that a child did not make a miscue for both samples. For Sample Two, a number between one to seven was assigned based on the type of miscue made; and for Sample One the number one was assigned to any type of miscue. For this analysis, for Sample Two, all numbers that were larger than one was assigned to be a value of one. This was necessary for the data to be analyzed alongside Sample One. Aver-age miscue rates across all participants were then calculated for each word and will be used as our dependent variable of interest.

##### Linguistic Analysis
In total, data on 942 words were collected, which were all part of 111 sentences. Of the 942 words, 312 content words had at least one other content word come before it within the same sentences.  These 312 content words were the focus of the following analysis. The SpaCy package was used to calculate semantic-similarity scores between target words and potential primer(s).
SpaCy calculates semantic similarity by utilizing a technique called word2vec, which essentially measures semantic similarity based on how often words appear in similar contexts [8]. One important note about SpaCy’s semantic similarity is that some antonyms that ap-pear in similar contexts (e.g., “love” and “hate”) will have a high semantic similarity score, despite being opposites in semantic mean-ing [14]. Even so, amongst antonyms, a semantic priming effect still exists [11]. 

##### *Paired Semantic-Similarity Scores*
To calculate paired semantic-similarity scores, semantic-similarity scores were calculated between the current target word and the most recent content word that preceded it. This was repeated across all content words within the passage. Stop words were filtered out by identifying them using SpaCy’s is_stop function. If a stop word was detected, they were assigned a paired semantic-similarity score of 0. Additionally, if a word was the first content-word within a passage (and thus, has no content-word before it), it was also as-signed a paired semantic-similarity score of 0. All words that had a paired semantic-similarity score of 0 was filtered out before analy-sis, since they were either stop words or content words in which calculating a paired semantic-similarity score was not possible. 
For a more specific example, we can examine how the sentence “Each night the mouse went into the kitchen.” would be processed. First, the word “each” is a stop word, and thus would be assigned a paired semantic-similarity score of 0. This is followed by “night”, which is the first content word to appear in the passage, and thus, would be assigned a score of 0. The word “the” is another stop word, which would be assigned a score of 0. Then, since the word “mouse” is a content word, the semantic similarity score between “mouse” and preceding content word “night” will be calculated, which will result in a paired semantic-similarity score of 0.16 for the word “mouse”. Then since the next word “went” is also a con-tent word, the semantic similarity score between “went” and the preceding content word “mouse” will be calculated, which will result in a paired semantic similarity score of 0.02 for the word “went”. This process was repeated throughout all passages to calcu-late paired semantic-similarity scores for all possible content words within the passages.

##### *Coherence Semantic-Similarity Scores*
To calculate coherence semantic-similarity scores, semantic similarity scores were calculated between the current target word and all prior content-words that preceded it within the same sentence. This was repeated across all content words within the passage. Once again, stop words were identified by using SpaCy’s is_stop func-tion, and were assigned a coherence semantic-similarity score of 0. And, if a word was the first content-word within a sentence, it was also assigned a coherence semantic-similarity score of 0, since there are no other content words that came before it within the same sen-tence. Once again, all words that had a coherence semantic-similarity score of 0 were filtered out before analysis, since they were either stop words or content words in which calculating a coherence semantic-similarity score was not possible. 
For another example, we can once again examine how the sentence “Each night the mouse went into the kitchen.” would be processed. Again, the word “each” would be recognized as a stop word and would be assigned a coherence semantic-similarity score of 0. Then, since the word “night” is the first content word to appear in the sentence, it would also be assigned a score of 0. The stop word “the” would be assigned a score of 0. Then, for the content word “mouse”, the semantic similarity score between “mouse” and the one preceding content word “night” would be calculated, which would result in a coherence semantic-similarity score of 0.16 for the word “mouse”. Then, since the next word “went” is also a content word, the semantic similarity scores between “went” and the two preceding content words (“mouse” and “night”) in the sentence will be calculated. Essentially, this means that the semantic similarity between “went” and “mouse” will be calculated first, followed by the semantic similarity between “went” and “night”. Then, these semantic similarity scores are averaged, resulting in a coherence semantic-similarity score of 0.17 for the word “went”. This process was repeated throughout all passages to calculate coherence seman-tic-similarity scores for all possible content words within the pas-sages.

##### *Coherence Semantic-Similarity Scores*
To calculate coherence semantic-similarity scores, semantic similari-ty scores were calculated between the current target word and all prior content-words that preceded it within the same sentence. This was repeated across all content words within the passage. Once again, stop words were identified by using SpaCy’s is_stop func-tion, and were assigned a coherence semantic-similarity score of 0. And, if a word was the first content-word within a sentence, it was also assigned a coherence semantic-similarity score of 0, since there are no other content words that came before it within the same sen-tence. Once again, all words that had a coherence semantic-similarity score of 0 were filtered out before analysis, since they were either stop words or content words in which calculating a coherence semantic-similarity score was not possible. 
For another example, we can once again examine how the sentence “Each night the mouse went into the kitchen.” would be processed. Again, the word “each” would be recognized as a stop word and would be assigned a coherence semantic-similarity score of 0. Then, since the word “night” is the first content word to appear in the sentence, it would also be assigned a score of 0. The stop word “the” would be assigned a score of 0. Then, for the content word “mouse”, the semantic similarity score between “mouse” and the one preceding content word “night” would be calculated, which would result in a coherence semantic-similarity score of 0.16 for the word “mouse”. Then, since the next word “went” is also a content word, the semantic similarity scores between “went” and the two preceding content words (“mouse” and “night”) in the sentence will be calculated. Essentially, this means that the semantic similarity between “went” and “mouse” will be calculated first, followed by the semantic similarity between “went” and “night”. Then, these semantic similarity scores are averaged, resulting in a coherence semantic-similarity score of 0.17 for the word “went”. This process was repeated throughout all passages to calculate coherence seman-tic-similarity scores for all possible content words within the pas-sages.

##### *Word Frequency and Word Length*
Additionally, word frequency scores were sourced from the SUB-TLEXus [3] corpus, which contains word frequency information on 74,286 words based on movie subtitles. The logarithmically transformed word frequency metric (“Lg10WF”) was used for analysis. Finally, word length was calculated for each word using Python, by measuring the length of each word with the built-in len() function. Both variables will be used as controls, since words are processed more quickly for high-frequency words [2] and words that have smaller word lengths [1]. Because of this, it is important to rule word length and word frequency effects as a pos-sible explanation for any potential priming effects.

##### Statistical Analysis
First, the correlation between all variables was analyzed. This was done to better understand the relationship between all variables, and to determine if there was high multicollinearity between any varia-bles. Coherence semantic-similarity scores and paired semantic-similarity scores had high multicollinearity – which was expected, since both measures are trying to capture semantic-relatedness be-tween target words and their potential primer(s). However, both independent variables will be analyzed in separate models when determining their significance in predicting word reading miscues – and thus, they were not dropped from analysis. None of the other independent variables had a high multicollinearity (R > 0.70) with another independent variable.  
For RQ1, analysis was conducted using multivariate linear regres-sion using R’s lm() function. Two separate regression models will be run for both the paired and coherence semantic-similarity varia-bles to determine if either variable significantly predicts word read-ing miscues after controlling for word length and word frequency. The first regression model will have average miscue rates as its dependent variable, and paired semantic-similarity scores, word length, and word frequency as its independent variables (paired semantic-similarity model). The second regression model will have average miscue rates as its dependent variable, and coherence se-mantic-similarity scores, word length, and word frequency as its independent variables (coherence semantic-similarity model). Sig-nificance, slope, and R2 will be reported. 
For RQ2, a full model that contains all variables of interest (paired semantic-similarity, coherence semantic-similarity, word length, and word frequency) that predicts word reading miscues will be run, and differences in t-statistics between coherence and paired seman-tic-similarity variables will be reported. Additionally, the full model will be compared with a model that either doesn’t include coherence semantic-similarity scores, or a model that doesn’t include paired semantic-similarity scores. Then, the differences between R2 be-tween the full model and the two models that do not have one of the variables of interest will be calculated. This would be a measure of the change in R2 when either the paired semantic-similarity or co-herence semantic-similarity variable is added to the full model last. The variable that results in the largest gain in R2 when added to the full model last would be the variable that explains the most unique variance in word reading miscues out of the two variables of inter-est (paired vs coherence semantic similarity). 



This repository contains all files, code, and passages used to investigate blurry boundaries in semantic priming effects. Data from participants are not publically available. If there are any questions about methodology, code, or data, the author of this repository can be reached at kenny.a.tang@vanderbilt.edu

