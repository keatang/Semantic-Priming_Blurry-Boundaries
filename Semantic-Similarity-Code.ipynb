{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bce3b65-9210-44cf-b628-0435b64b1180",
   "metadata": {},
   "source": [
    "# Data Wrangling and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce65d6e3-4a6d-4aca-9982-ce8569f984de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here are the packages I'll be using:\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "#load in spacy nlp stuff\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "df_SUBTlog10 = pd.read_csv('data/word_metrics/SUBTLEXus_log10.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "035683d1-8d32-4a56-8686-7d9995770758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('passages/air.txt'), WindowsPath('passages/mouse_in_the_house.txt'), WindowsPath('passages/the_brain_and_five_senses.txt'), WindowsPath('passages/the_surprise.txt'), WindowsPath('passages/toads.txt')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#create a path object that we can loop through to access the text files\n",
    "reading_dir = Path('passages') #using Jupyter Lab - default dir is folder code is in\n",
    "\n",
    "#create a list of reading files to be looped through later\n",
    "reading_files = list(reading_dir.glob('*.txt'))\n",
    "\n",
    "print(reading_files) #Just checking the output\n",
    "\n",
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40029bb4-edfb-40c5-a01f-1f7010daff5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Air is all around us. But we can't see it. How do we know it is there? There are many ways. We can see what air does. Moving air is called wind. Wind moves plants. Wind moves dirt. Strong winds can move heavy things. Strong winds can even move a house. We can weigh air. We can weigh two balloons. The one with a lot of air weighs more. We can see what air does. We can weigh air. Then we know it is there. \n"
     ]
    }
   ],
   "source": [
    "text = reading_files[0].read_text(encoding='utf-8') #making sure that path object works as it should\n",
    "print(text) #Checking that there are no strange formatting errors within text.\n",
    "\n",
    "nlp_text = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d5d0629-74c4-475b-adba-e6293ffecbc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air.txt\n",
      "first content word found: air 1\n",
      "mouse_in_the_house.txt\n",
      "first content word found: mouse 1\n",
      "the_brain_and_five_senses.txt\n",
      "first content word found: people 1\n",
      "the_surprise.txt\n",
      "first content word found: sam 1\n",
      "toads.txt\n",
      "first content word found: toads 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tangka\\AppData\\Local\\Temp\\ipykernel_11336\\3833608638.py:37: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  sing_prior_sim = curr_word.similarity(prior_word) #This calculates the semantic similarity between the \"target\" word (curr_word) and \"primer\" word (prior_word)\n",
      "C:\\Users\\tangka\\AppData\\Local\\Temp\\ipykernel_11336\\3833608638.py:66: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  similarity_score = curr_word.similarity(before_m[k]) #Calculate semantic similarity score between target word and primer word\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>prior_sim_val</th>\n",
       "      <th>single_primer</th>\n",
       "      <th>word_len</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>air</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>air.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>is</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>air.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>all</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>air.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>around</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>air.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>us</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>air.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>305</td>\n",
       "      <td>toads</td>\n",
       "      <td>0.192754</td>\n",
       "      <td>0.113033</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>toads.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>306</td>\n",
       "      <td>in</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>toads.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>307</td>\n",
       "      <td>gardens</td>\n",
       "      <td>0.131438</td>\n",
       "      <td>0.314948</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>toads.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>308</td>\n",
       "      <td>and</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>toads.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>309</td>\n",
       "      <td>parks</td>\n",
       "      <td>0.199556</td>\n",
       "      <td>0.604887</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>toads.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>942 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     word_id     word  prior_sim_val  single_primer  word_len  sent_num  \\\n",
       "0          1      air       0.000000       0.000000         3         1   \n",
       "1          2       is       0.000000       0.000000         2         1   \n",
       "2          3      all       0.000000       0.000000         3         1   \n",
       "3          4   around       0.000000       0.000000         6         1   \n",
       "4          5       us       0.000000       0.000000         2         1   \n",
       "..       ...      ...            ...            ...       ...       ...   \n",
       "937      305    toads       0.192754       0.113033         5        27   \n",
       "938      306       in       0.000000       0.000000         2        27   \n",
       "939      307  gardens       0.131438       0.314948         7        27   \n",
       "940      308      and       0.000000       0.000000         3        27   \n",
       "941      309    parks       0.199556       0.604887         5        27   \n",
       "\n",
       "       passage  \n",
       "0      air.txt  \n",
       "1      air.txt  \n",
       "2      air.txt  \n",
       "3      air.txt  \n",
       "4      air.txt  \n",
       "..         ...  \n",
       "937  toads.txt  \n",
       "938  toads.txt  \n",
       "939  toads.txt  \n",
       "940  toads.txt  \n",
       "941  toads.txt  \n",
       "\n",
       "[942 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################\n",
    "## Creating a DataFrame that\n",
    "## has all passages tokenized\n",
    "## and associated with a parts-of-speech tag\n",
    "######################################\n",
    "\n",
    "pos_tuple = [] #This will be used to create a df, and will contain a tuple of all variables of interest\n",
    "\n",
    "for i, f in enumerate(reading_files):\n",
    "    sent_num = 0\n",
    "    print(f.name) #Print which file we're working on\n",
    "    text = f.read_text(encoding='utf-8') #read the files and save the texts\n",
    "    text = text.lower() #Lowercase all of the text\n",
    "    nw = 0 #This keeps track of the order of words within the passage\n",
    "    #Let's go ahead and tokenize the text\n",
    "    punct_text = nlp(text) #Tokenize text\n",
    "    filename = f.name #Read filename\n",
    "    \n",
    "    tokenized_text = [] #just a list of all of the words that have been nlp'd, with both punct and spaces filtered out\n",
    "    first_cont_found = 0 #I want this to reset everytime a new passage is analyzed -- a check to see if the first content word has been found\n",
    "    \n",
    "    for word in punct_text:#loop through the tokenized text and get rid of all punctuation and spaces\n",
    "        if not word.is_punct and not word.pos_ == \"SPACE\":\n",
    "            tokenized_text.append(word)\n",
    "    \n",
    "    #The code below calculates the \"paired semantic-similarity\" score, or the semantic-similarity score between a target-word and the most-recent content word.\n",
    "    for i in range(len(tokenized_text)): #Loop through list of tokenized words\n",
    "        if not tokenized_text[i].is_stop: \n",
    "            if first_cont_found == 0: #If current word is NOT a stop word, and the first stop word has not been found yet\n",
    "                prior_word = tokenized_text[i] #This lets us get our first \"prior\" content word -- essentially, this will be our \"primer\" word\n",
    "                first_cont_found = 1 #This stops this loop from occuring again for this passage once the first content word is found\n",
    "                sing_prior_sim = 0 #Since this first content word can't be compared to anything, 0 is the \"paired semantic-similarity\" score\n",
    "                print(\"first content word found:\", prior_word, first_cont_found)\n",
    "            elif first_cont_found == 1: #Now, for all following non-stop words, I can just do this:\n",
    "                curr_loc = i #This will increase by one as I loop through each word within the passage, and keeps track of which word is the current \"target\" word\n",
    "                curr_word = tokenized_text[i] #This assigns the current \"target\" word to variable curr_word\n",
    "                sing_prior_sim = curr_word.similarity(prior_word) #This calculates the semantic similarity between the \"target\" word (curr_word) and \"primer\" word (prior_word)\n",
    "                #print(\"content words can be compared:\",curr_word, prior_word, first_func_found, loc_dist)\n",
    "                prior_word = curr_word #Now the \"target\" word will become the \"primer\" word for the next content word in the passage\n",
    "        else: \n",
    "            #If none are true -- current word IS a stop word\n",
    "            sing_prior_sim = 0 #assign it to be 0 -- this will identify this word as a stop word or a content word in which a paired semantic-similarity score was impossible to calculate.\n",
    "            #All words with a value of 0 for this variable will be filtered out during analysis.\n",
    "        \n",
    "        if tokenized_text[i].is_sent_start: #Only triggers on the first word of a sentence\n",
    "            word_list = [] #This essentially stores all words from the current sentence\n",
    "            #And, it also resets when a new sentence is detected.\n",
    "            sent_num += 1 #This keeps track of which sentence we're currently in.\n",
    "            m = 0 #This is later used to keep track of which word in the sentence we are analyzing as the \"target\" word. It is here, because we want it to reset at the start of every new sentence.\n",
    "            word_list.append(tokenized_text[i]) #Basically, append the first word of the sentence to our array of all words within a sentence.\n",
    "            \n",
    "            index = i+1 #This is a counter that lets us loop through the passage until a new sentence is detected\n",
    "            while tokenized_text[index].is_sent_start == False: #While a new sentence hasn't been detected...\n",
    "                word_list.append(tokenized_text[index]) #Append the rest of the words in the current sentence of interest to our sentence word list\n",
    "                if index < len(tokenized_text)-1: #Makes sure we don't have an index error\n",
    "                    index += 1 #This allows us to move onto the next word\n",
    "                else: #If the above condition is false, we need to break out of this while loop because we have reached the end of the passage.\n",
    "                    break;\n",
    "\n",
    "        ## The code below allows us to calculate the \"coherence semantic-similarity\" score, or the semantic-similarity score between a target-word and all preceding content words within a sentence.\n",
    "        prior_sem_sim = [] #This keeps track of all semantic similarity calculations made for any given target word. And resets for each word.\n",
    "        before_m = word_list[:m] #Basically, this returns of word list of all items that come before the target word at index m\n",
    "        curr_word = word_list[m] #Save the current word we're looking at -- this is our \"target\" word.\n",
    "        for k in range(len(before_m)): #Loop through all words that appeared before the target word at index m\n",
    "            if not curr_word.is_stop and not before_m[k].is_stop: #I only want to calculate semantic similarity scores between the target word and words that are NOT stop words\n",
    "                similarity_score = curr_word.similarity(before_m[k]) #Calculate semantic similarity score between target word and primer word\n",
    "                prior_sem_sim.append(similarity_score) #Append it onto our prior_sem_sim variable, and repeat until there are no more content words that appeared before the target word in the sentence.\n",
    "        m += 1 #Update m, so that the next loop through, it makes these comparasions with the next word in the word list\n",
    "        if len(prior_sem_sim) > 0: #Basically checks to make sure that there were actually content words before the target word within the sentence.\n",
    "            prior_sim_val = sum(prior_sem_sim)/len(prior_sem_sim) #If there are, take the average of all semantic-similarity score in order to get the \"coherence semantic similarity\" score.\n",
    "            prior_sem_sim = [] #Reset the value for prior_sem_sim for the next word\n",
    "        else: #If word is part of a sentence with no content words or just one...\n",
    "            prior_sim_val = 0 #Assign sem_sim_val to be 0 if the word is a stop word, or a content word with no preceding content word within the same sentence.\n",
    "            #Any word with a prior_sim_val of 0 will be filtered out during analysis.\n",
    "        \n",
    "        if not tokenized_text[i].is_punct and not tokenized_text[i].pos_ == \"SPACE\": #If the current token is not punctuation or a space\n",
    "            nw += 1 #Increase the word count by one\n",
    "            pos_tuple.append((nw,tokenized_text[i].text, prior_sim_val, sing_prior_sim, len(tokenized_text[i].text), sent_num, filename)) #Append variables into a list of tuples\n",
    "\n",
    "#Finally, use the list of tuples we just created to make a dataframe with all of our words and variable scores\n",
    "pos_df = pd.DataFrame(pos_tuple, columns =['word_id','word','prior_sim_val', 'single_primer', 'word_len','sent_num','passage'])\n",
    "\n",
    "pos_df.to_csv(r'pos_Output.csv', index=False) #Export dataframe for sanity check.\n",
    "\n",
    "pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4739073c-2306-4c70-85bf-8a385546becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                    #\n",
    "##Importing SUBTLEXus word frequency##\n",
    "#           (standardized)           #\n",
    "df_SUBTlog10['Word'] = df_SUBTlog10['Word'].str.lower()\n",
    "df_SUBTlog10['Word'] = df_SUBTlog10['Word'].str.replace('\\W', '', regex=True)\n",
    "\n",
    "#Sum all duplicates\n",
    "df_SUBTlog10['TotalLog10'] = df_SUBTlog10.groupby('Word')['Lg10WF'].transform('sum')\n",
    "df_SUBTlog10['TotalSUBTLWF'] = df_SUBTlog10.groupby('Word')['SUBTLWF'].transform('sum')\n",
    "\n",
    "#Drop all duplicates\n",
    "df_ndSUBTlog10 = df_SUBTlog10.drop_duplicates('Word')\n",
    "\n",
    "#Only take the columns I want\n",
    "df_ndSUBTlog10 = df_SUBTlog10[['Word','TotalLog10', 'TotalSUBTLWF']]\n",
    "df_ndSUBTlog10.columns = ['word', 'totallog10', 'totalsubtlwf']\n",
    "\n",
    "#Merge dfs\n",
    "pos_df = pos_df.merge(df_ndSUBTlog10, on = 'word', how = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a09419a0-b9c4-411c-9f04-c231891a22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####                     ####\n",
    "## MY FUNCTIONS LIVE HERE ###\n",
    "####                     ####\n",
    "\n",
    "##REORGANIZING TO FIX WHEN WORDS ARE SPLIT INTO TWO TOKENS##\n",
    "## ie she'll -> she|'ll ##\n",
    "## Basically, split the word into two columns, with both columns having the same data ##\n",
    "## So if a kid had a miscue on \"she'll\", that represents a miscue on both tokens \"she\" and \"'ll\"\n",
    "## This fix is necessary, because the data with miscue rates have words that are not split in this way\n",
    "## If this isn't fixed, merging the dataframe with all of our paired and coherence semantic-similarity scores with the dataframe with miscue rate data would not be possible\n",
    "\n",
    "def apst_sort(text_data):\n",
    "    index_move_from = []\n",
    "    index_move_to = []\n",
    "    \n",
    "    for (columnName, columnData) in text_data.items():\n",
    "        #print(columnName) checking to make sure loop ends at original df end-point\n",
    "        #print(columnData[0])\n",
    "        \n",
    "        #Just a backup check to make sure everything being run through nlp() is a string\n",
    "        if type(columnData[0]) == str:\n",
    "            nlp_word = nlp(columnData[0])\n",
    "        \n",
    "            if len(nlp_word) > 1:\n",
    "                print(columnData[0])\n",
    "                word_parts = []\n",
    "                #duplicate column and append to end of df\n",
    "                text_data[len(text_data.columns)] = text_data[columnName]\n",
    "\n",
    "                #This is to fix cases where the word that's being split is due to a - \n",
    "                for token in nlp_word:\n",
    "                    if not token.is_punct and not token.pos_ == \"SPACE\":\n",
    "                        word_parts.append(token.text)\n",
    "\n",
    "                count = 0 #count to keep track of the two word parts\n",
    "                for part in word_parts:\n",
    "                    if count == 0:\n",
    "                        #for the first part, append to where current word is\n",
    "                        text_data[columnName][0] = part\n",
    "                        index_move_to.append(columnName) #Keep track of where the current word is\n",
    "                        count += 1\n",
    "                    elif count == 1:\n",
    "                        #Replace word of the newly added column to the second half of word\n",
    "                        text_data.iloc[0, (len(text_data.columns)-1)] = part\n",
    "                        index_move_from.append(len(text_data.columns)-1) #Keep track of where the added part of the word is\n",
    "                        count += 1\n",
    "\n",
    "    #Now that the new columns are added, we want it to match with the word_ids in our POS_word list\n",
    "    #This means we need to reorganize the columns to match (we use the columns for word_ids, and since the split words are at the end of the df\n",
    "    #we need to move them to their rightful place -- right after the word they were split from)\n",
    "\n",
    "    new_col_names = np.arange(len(text_data.columns)) #this is what the new column names are about to be\n",
    "\n",
    "    for i in range(len(index_move_to)): #So for however many instances of words being split through tokenization\n",
    "        #replace the col_name of where the part of the word currently is \n",
    "        #with where it SHOULD be (+i because with each loop, the col_names are being shifted)\n",
    "        new_col_names[index_move_from[i]] = index_move_to[i] + 1 + i \n",
    "        #this is a little complicated, basically, only add one to values between where we want to insert our split word, and where the split word currently lives\n",
    "        new_col_names[(index_move_to[i]+1):(len(new_col_names)-len(index_move_to))] = new_col_names[(index_move_to[i]+1):(len(new_col_names)-len(index_move_to))]+1 \n",
    "\n",
    "    text_data.columns = new_col_names #now replace the columns with our newly ordered names\n",
    "    text_data.sort_index(axis=1, inplace=True) #and now let's sort it so that it's back to numerical order (PHEW!)\n",
    "    return(text_data)\n",
    "\n",
    "##CREATE REFEREANT TABLE TO ASSIST WITH MERGING POS_DF WITH DATA_DF\n",
    "def ref_create(text_data, passage):\n",
    "    #Creating a referant dataframe\n",
    "    word_index = pd.DataFrame(columns = ['word', 'word_id', 'passage'])\n",
    "    #Put the words in the word column\n",
    "    word_index['word'] = text_data.iloc[0].to_frame()\n",
    "    #Set the column names as the word_id column\n",
    "    word_index['word_id'] = text_data.columns #np.arange(len(word_index))\n",
    "    #Set the passages (so that we can later use that information to merge dataframes)\n",
    "    word_index['passage'] = passage #'toads.csv' ###!!!!! THIS WILL NEED TO BE CHANGED TO FILENAME !!!! ####\n",
    "    #Get rid of the first row, which is just Word and index 0 (not actually words from the passage)\n",
    "    word_index = word_index.drop([0]) #Dropping that extra row that had \n",
    "                                      #\"word\" as word, and 0 as index\n",
    "    #lowercase everything \n",
    "    word_index['word'] = word_index['word'].str.lower()\n",
    "    return(word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6908ed6b-366e-4aa5-8c56-38b74c55b3e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air.csv\n",
      "can't\n",
      "mouse_in_the_house.csv\n",
      "didn't\n",
      "224-3414\n",
      "don't\n",
      "we'll\n",
      "the_brain_and_five_senses.csv\n",
      "the_surprise.csv\n",
      "Sam's\n",
      "didn't\n",
      "Sam's\n",
      "I'll\n",
      "didn't \n",
      "toads.csv\n",
      "toad's\n",
      "hunter's\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>word_id</th>\n",
       "      <th>miscue</th>\n",
       "      <th>word</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4140</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>air</td>\n",
       "      <td>air.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4139</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>air</td>\n",
       "      <td>air.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4138</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>air</td>\n",
       "      <td>air.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>air</td>\n",
       "      <td>air.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4136</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>air</td>\n",
       "      <td>air.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43873</th>\n",
       "      <td>RC_227</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>parks</td>\n",
       "      <td>toads.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43874</th>\n",
       "      <td>RC_228</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>parks</td>\n",
       "      <td>toads.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43875</th>\n",
       "      <td>RC_267</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>parks</td>\n",
       "      <td>toads.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43876</th>\n",
       "      <td>RC_268</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>parks</td>\n",
       "      <td>toads.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43877</th>\n",
       "      <td>RC_270</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>parks</td>\n",
       "      <td>toads.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132498 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID word_id miscue   word    passage\n",
       "0        4140       1      0    air    air.csv\n",
       "1        4139       1      0    air    air.csv\n",
       "2        4138       1      0    air    air.csv\n",
       "3        4137       1      0    air    air.csv\n",
       "4        4136       1    NaN    air    air.csv\n",
       "...       ...     ...    ...    ...        ...\n",
       "43873  RC_227     309      0  parks  toads.csv\n",
       "43874  RC_228     309      0  parks  toads.csv\n",
       "43875  RC_267     309      0  parks  toads.csv\n",
       "43876  RC_268     309      0  parks  toads.csv\n",
       "43877  RC_270     309      0  parks  toads.csv\n",
       "\n",
       "[132498 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############\n",
    "## This code here converts the csvs that contain\n",
    "## data on miscue/correct cues into a format that\n",
    "## can be merged with the pos data-table created above.\n",
    "##\n",
    "## Data had to be converted from wide to long using\n",
    "## the melt function. And fixes had to be made to words\n",
    "## with an apostrophe in order to succesfully match the tokenized versions of those\n",
    "## words needed to succesfully merge the two dfs together.\n",
    "##############\n",
    "\n",
    "#create a path object that we can loop through to access the text files\n",
    "reading_dir = Path('data') #using Jupyter Lab - default dir is folder code is in\n",
    "\n",
    "#create a list of reading files to be looped through later\n",
    "reading_files = list(reading_dir.glob('*.csv'))\n",
    "\n",
    "#Create the final df of this done on ALL passages and just stacked on top of each other\n",
    "merged_stack = pd.DataFrame(columns = [\"ID\", \"word_id\", \"miscue\", \"word\",\"passage\"])\n",
    "\n",
    "\n",
    "#How to read multiple file in at once\n",
    "for file in reading_files:\n",
    "    csv_data = pd.read_csv(file, dtype=str, header = None)\n",
    "    csv_name = file.name\n",
    "    print(csv_name)\n",
    "    \n",
    "    csv_data.iloc[0,0] = 'Words'\n",
    "\n",
    "    ##REORGANIZING TO FIX WHEN WORDS ARE SPLIT INTO TWO TOKENS##\n",
    "    csv_data = apst_sort(csv_data)\n",
    "\n",
    "    #Creating a referant dataframe\n",
    "    word_index = ref_create(csv_data, csv_name)\n",
    "\n",
    "    #Cleaning the row with just words out of the original df (so that we can melt it)\n",
    "    csv_data = csv_data.drop(0)\n",
    "\n",
    "    #This part is kind of strange, but it works, so I won't complain\n",
    "    #Create a list of all columns as string\n",
    "    list_str = [str(x) for x in csv_data.columns]\n",
    "    #Actually name all csv_data columns into stringified indexes (melting doesn't work unless we do this)\n",
    "    csv_data.columns = list_str\n",
    "\n",
    "    #Now drop that extra column (because we'll need to use this when melting the df)\n",
    "    #we don't want value at index 0, because that's our id_vars\n",
    "    list_str.pop(0)\n",
    "\n",
    "    #let's melt the data\n",
    "    csv_data_long = pd.melt(csv_data, id_vars = '0', value_vars = list_str)\n",
    "    csv_data_long.columns = ['ID', 'word_id', 'miscue'] #renaming columns for merging\n",
    "    #Turning the word_id column to numpy int32 type, so that merging can happen with word_index\n",
    "    csv_data_long['word_id'] = csv_data_long['word_id'].astype(np.int32)\n",
    "\n",
    "\n",
    "    csv_data_merged = csv_data_long.merge(word_index, on = 'word_id', how = 'left')\n",
    "    \n",
    "    merged_stack = pd.concat([merged_stack,csv_data_merged])\n",
    "    \n",
    "    #csv_data_merged # IT WORKS!!!\n",
    "    word_index.to_csv('WordList_' + csv_name)\n",
    "\n",
    "merged_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef24a5bf-2184-410a-8123-02dd24c41bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tangka\\AppData\\Local\\Temp\\ipykernel_11336\\1544540147.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_stack['word'] = merged_stack['word'].str.strip()\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "#MORE GENERAL DATA CLEANING\n",
    "#Removing NAs, white spaces on words, and cleaning dtypes\n",
    "#for better merge compatability.\n",
    "#\n",
    "#Additionally, cleaning passages column:\n",
    "#\"passages.csv/txt\" to just be \"passage\" for\n",
    "#better merge compatability\n",
    "#######\n",
    "\n",
    "#Drop all words that have NA cues (not all participants did every passage)\n",
    "merged_stack = merged_stack.dropna()\n",
    "\n",
    "#Remove white spaces improperly added from original csv\n",
    "merged_stack['word'] = merged_stack['word'].str.strip()\n",
    "\n",
    "#Converting word_id and miscues to int (they aren't strings)\n",
    "merged_stack = merged_stack.astype({'word_id': 'int', 'miscue':'int'})\n",
    "\n",
    "#We want to drop everything after period in the \"passage\" column\"\n",
    "#We'll be merging based off of matches on word_id, word, and passage, so this is a necessary step\n",
    "merged_stack[['passage','file_type']] = merged_stack['passage'].str.split(\".\",expand=True)\n",
    "pos_df[['passage','file_type']] = pos_df['passage'].str.split(\".\", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43be3e01-51a6-4578-830a-c74d75f1f3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Checking to make sure the passage labels match\n",
    "print(set(merged_stack['passage']) == set(pos_df['passage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41d4921e-06f4-402d-b3bb-7e5f142e65dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This merges the pos_data established earlier with the \n",
    "## raw miscue/correct cue data that was cleaned and converted\n",
    "## from wide to long\n",
    "\n",
    "#Let's drop the unnecessary file_type column\n",
    "merged_stack = merged_stack.drop(['file_type'], axis=1)\n",
    "pos_df = pos_df.drop(['file_type'], axis=1)\n",
    "\n",
    "##Time to merge the data!\n",
    "final_merge = pd.merge(merged_stack, pos_df, how = 'left', left_on=['word_id','word','passage'], right_on=['word_id','word','passage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bfce937-6b42-4988-b863-6ca7b4632172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some more data cleaning\n",
    "\n",
    "#We don't care about what kind of error they made, so let's just make any value that's 1 or above just 1.\n",
    "final_merge.loc[final_merge['miscue'] > 1, 'miscue'] = 1\n",
    "\n",
    "#Just a sanity check\n",
    "#final_merge[final_merge['ID']=='4119'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e58f1c07-3213-4414-a9fe-bbb35667976f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>word_id</th>\n",
       "      <th>miscue</th>\n",
       "      <th>word</th>\n",
       "      <th>passage</th>\n",
       "      <th>prior_sim_val</th>\n",
       "      <th>single_primer</th>\n",
       "      <th>word_len</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>totallog10</th>\n",
       "      <th>totalsubtlwf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4140</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>air</td>\n",
       "      <td>air</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8507</td>\n",
       "      <td>139.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4139</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>air</td>\n",
       "      <td>air</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8507</td>\n",
       "      <td>139.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4138</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>air</td>\n",
       "      <td>air</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8507</td>\n",
       "      <td>139.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>air</td>\n",
       "      <td>air</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8507</td>\n",
       "      <td>139.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4133</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>air</td>\n",
       "      <td>air</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8507</td>\n",
       "      <td>139.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85429</th>\n",
       "      <td>RC_227</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>parks</td>\n",
       "      <td>toads</td>\n",
       "      <td>0.199556</td>\n",
       "      <td>0.604887</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>2.2810</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85430</th>\n",
       "      <td>RC_228</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>parks</td>\n",
       "      <td>toads</td>\n",
       "      <td>0.199556</td>\n",
       "      <td>0.604887</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>2.2810</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85431</th>\n",
       "      <td>RC_267</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>parks</td>\n",
       "      <td>toads</td>\n",
       "      <td>0.199556</td>\n",
       "      <td>0.604887</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>2.2810</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85432</th>\n",
       "      <td>RC_268</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>parks</td>\n",
       "      <td>toads</td>\n",
       "      <td>0.199556</td>\n",
       "      <td>0.604887</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>2.2810</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85433</th>\n",
       "      <td>RC_270</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>parks</td>\n",
       "      <td>toads</td>\n",
       "      <td>0.199556</td>\n",
       "      <td>0.604887</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>2.2810</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85434 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  word_id  miscue   word passage  prior_sim_val  single_primer  \\\n",
       "0        4140        1       0    air     air       0.000000       0.000000   \n",
       "1        4139        1       0    air     air       0.000000       0.000000   \n",
       "2        4138        1       0    air     air       0.000000       0.000000   \n",
       "3        4137        1       0    air     air       0.000000       0.000000   \n",
       "4        4133        1       0    air     air       0.000000       0.000000   \n",
       "...       ...      ...     ...    ...     ...            ...            ...   \n",
       "85429  RC_227      309       0  parks   toads       0.199556       0.604887   \n",
       "85430  RC_228      309       0  parks   toads       0.199556       0.604887   \n",
       "85431  RC_267      309       0  parks   toads       0.199556       0.604887   \n",
       "85432  RC_268      309       0  parks   toads       0.199556       0.604887   \n",
       "85433  RC_270      309       0  parks   toads       0.199556       0.604887   \n",
       "\n",
       "       word_len  sent_num  totallog10  totalsubtlwf  \n",
       "0             3         1      3.8507        139.02  \n",
       "1             3         1      3.8507        139.02  \n",
       "2             3         1      3.8507        139.02  \n",
       "3             3         1      3.8507        139.02  \n",
       "4             3         1      3.8507        139.02  \n",
       "...         ...       ...         ...           ...  \n",
       "85429         5        27      2.2810          3.73  \n",
       "85430         5        27      2.2810          3.73  \n",
       "85431         5        27      2.2810          3.73  \n",
       "85432         5        27      2.2810          3.73  \n",
       "85433         5        27      2.2810          3.73  \n",
       "\n",
       "[85434 rows x 11 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(pos_df[pos_df['contain_rel'] == 1]),len(pos_df)) \n",
    "#Export it (just so I can use a different analysis software later if need be)\n",
    "final_merge.to_csv('to_be_analyzed.csv')\n",
    "final_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b71179f-d60e-4736-aaca-11c4cd7f8361",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "558cdc8e-cf7b-4231-92db-ae350835e0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_id</th>\n",
       "      <th>word</th>\n",
       "      <th>passage</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>miscue</th>\n",
       "      <th>prior_sim_val</th>\n",
       "      <th>single_primer</th>\n",
       "      <th>word_len</th>\n",
       "      <th>totallog10</th>\n",
       "      <th>totalsubtlwf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>air</td>\n",
       "      <td>air</td>\n",
       "      <td>1</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.8507</td>\n",
       "      <td>139.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>all</td>\n",
       "      <td>the_brain_and_five_senses</td>\n",
       "      <td>1</td>\n",
       "      <td>0.075949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.4204</td>\n",
       "      <td>5161.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>once</td>\n",
       "      <td>mouse_in_the_house</td>\n",
       "      <td>1</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.2453</td>\n",
       "      <td>344.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>sam</td>\n",
       "      <td>the_surprise</td>\n",
       "      <td>1</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.7981</td>\n",
       "      <td>123.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>toads</td>\n",
       "      <td>toads</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.4914</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>305</td>\n",
       "      <td>toads</td>\n",
       "      <td>toads</td>\n",
       "      <td>27</td>\n",
       "      <td>0.035211</td>\n",
       "      <td>0.192754</td>\n",
       "      <td>0.113033</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.4914</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>306</td>\n",
       "      <td>in</td>\n",
       "      <td>toads</td>\n",
       "      <td>27</td>\n",
       "      <td>0.035211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.6976</td>\n",
       "      <td>9773.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>307</td>\n",
       "      <td>gardens</td>\n",
       "      <td>toads</td>\n",
       "      <td>27</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.131438</td>\n",
       "      <td>0.314948</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.3160</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>308</td>\n",
       "      <td>and</td>\n",
       "      <td>toads</td>\n",
       "      <td>27</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8343</td>\n",
       "      <td>13387.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>309</td>\n",
       "      <td>parks</td>\n",
       "      <td>toads</td>\n",
       "      <td>27</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>0.199556</td>\n",
       "      <td>0.604887</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.2810</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>942 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     word_id     word                    passage  sent_num    miscue  \\\n",
       "0          1      air                        air         1  0.169811   \n",
       "1          1      all  the_brain_and_five_senses         1  0.075949   \n",
       "2          1     once         mouse_in_the_house         1  0.169811   \n",
       "3          1      sam               the_surprise         1  0.096386   \n",
       "4          1    toads                      toads         1  0.021127   \n",
       "..       ...      ...                        ...       ...       ...   \n",
       "937      305    toads                      toads        27  0.035211   \n",
       "938      306       in                      toads        27  0.035211   \n",
       "939      307  gardens                      toads        27  0.056338   \n",
       "940      308      and                      toads        27  0.042254   \n",
       "941      309    parks                      toads        27  0.070423   \n",
       "\n",
       "     prior_sim_val  single_primer  word_len  totallog10  totalsubtlwf  \n",
       "0         0.000000       0.000000       3.0      3.8507        139.02  \n",
       "1         0.000000       0.000000       3.0      5.4204       5161.86  \n",
       "2         0.000000       0.000000       4.0      4.2453        344.88  \n",
       "3         0.000000       0.000000       3.0      3.7981        123.16  \n",
       "4         0.000000       0.000000       5.0      1.4914          0.59  \n",
       "..             ...            ...       ...         ...           ...  \n",
       "937       0.192754       0.113033       5.0      1.4914          0.59  \n",
       "938       0.000000       0.000000       2.0      5.6976       9773.41  \n",
       "939       0.131438       0.314948       7.0      2.3160          4.04  \n",
       "940       0.000000       0.000000       3.0      5.8343      13387.84  \n",
       "941       0.199556       0.604887       5.0      2.2810          3.73  \n",
       "\n",
       "[942 rows x 10 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This allows us to get average miscue rates for each word\n",
    "grouped_data = final_merge.groupby(by=['word_id', 'word', 'passage','sent_num'], group_keys = True).mean(numeric_only = True)\n",
    "t_data = grouped_data.reset_index()\n",
    "\n",
    "t_data.to_csv('t-data.csv') #Converting to csv just so I can double-check\n",
    "t_data #Data looks good!\n",
    "\n",
    "#miscue column = average miscue rates across participants on that word\n",
    "#prior_sim_val = \"coherence semantic-similarity\" scores, essentially, it's the average semantic-similarity score between the target word and \n",
    "                 #all preceding content words that appeared within the same sentence.\n",
    "#single_primer = \"paired semantic-similarity\" scores, essentially it's the semantic-similarity score between the target word and \n",
    "                 #the closest preceding content word that appeared within the same passage.\n",
    "#word_len = That's just the word length of a word.\n",
    "#totallog10 = This is word frequency that has been logorithmicaly transformed.\n",
    "#totalsubtlwf = This is the raw word frequency (will not be included in analysis, since totallog10 is likely a better (and more linear) measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "add55aee-d67d-4d29-a2d7-c1f6dad6e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code below allows us to filter out stop words as well as content words that did not have a preceding content word in the passage/sentence\n",
    "\n",
    "#The single_primer is our varaible for \"paired semantic-similarity\" scores. \n",
    "t_data = t_data[t_data['single_primer'] != 0] #If it = 0, that means it's either a stop word, or the first content word of a passage.\n",
    "#We want to exclude that from our analysis, since we didn't make semantic-similarity comparsions for these words.\n",
    "\n",
    "#The prior_sim_val is our variable for \"coherence semantic-similarity\" scores.\n",
    "t_data_prior = t_data[t_data['prior_sim_val']!=0] #If it = 0, that means it's either a stop word, or the first content word of a sentence.\n",
    "#We want to exclude that from our analysis, since we didn't make semantic-similarity comparsions for these words.\n",
    "\n",
    "#Export it for further analysis in R.\n",
    "t_data.to_csv(\"single_R_Data.csv\")\n",
    "t_data_prior.to_csv(\"prior_R_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "536cffe6-2537-4b17-8774-b8b91f1bc652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code takes a chunk of t_data (just our variables of interest) to be used for calculating correlations.\n",
    "spruced_data = t_data_prior[['miscue', 'prior_sim_val', 'single_primer', 'word_len', 'totallog10']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bac8812-ccd6-4cbf-9259-1f1caeb785c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>miscue</th>\n",
       "      <th>prior_sim_val</th>\n",
       "      <th>single_primer</th>\n",
       "      <th>word_len</th>\n",
       "      <th>totallog10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.084337</td>\n",
       "      <td>0.081770</td>\n",
       "      <td>0.081770</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.6954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.194954</td>\n",
       "      <td>0.194954</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.3053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.126582</td>\n",
       "      <td>0.403196</td>\n",
       "      <td>0.403196</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.6365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.077003</td>\n",
       "      <td>0.206933</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.1929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.075949</td>\n",
       "      <td>0.206220</td>\n",
       "      <td>0.206220</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300969</td>\n",
       "      <td>0.300969</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.3589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.135396</td>\n",
       "      <td>0.173675</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.4847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>0.035211</td>\n",
       "      <td>0.192754</td>\n",
       "      <td>0.113033</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.131438</td>\n",
       "      <td>0.314948</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.3160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>0.070423</td>\n",
       "      <td>0.199556</td>\n",
       "      <td>0.604887</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.2810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       miscue  prior_sim_val  single_primer  word_len  totallog10\n",
       "12   0.084337       0.081770       0.081770       8.0      3.6954\n",
       "18   0.007042       0.194954       0.194954       5.0      4.3053\n",
       "23   0.126582       0.403196       0.403196       6.0      2.6365\n",
       "30   0.060241       0.077003       0.206933       4.0      4.1929\n",
       "36   0.075949       0.206220       0.206220       4.0      4.0531\n",
       "..        ...            ...            ...       ...         ...\n",
       "932  0.000000       0.300969       0.300969       6.0      4.3589\n",
       "935  0.021127       0.135396       0.173675       3.0      3.4847\n",
       "937  0.035211       0.192754       0.113033       5.0      1.4914\n",
       "939  0.056338       0.131438       0.314948       7.0      2.3160\n",
       "941  0.070423       0.199556       0.604887       5.0      2.2810\n",
       "\n",
       "[310 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spruced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d68ebb0-1007-4ba8-b38c-0400b660da77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 miscue  prior_sim_val  single_primer  word_len  totallog10\n",
      "miscue         1.000000      -0.086004      -0.087655  0.072306   -0.103650\n",
      "prior_sim_val -0.086004       1.000000       0.773533  0.290319   -0.123401\n",
      "single_primer -0.087655       0.773533       1.000000  0.226175   -0.097613\n",
      "word_len       0.072306       0.290319       0.226175  1.000000   -0.362521\n",
      "totallog10    -0.103650      -0.123401      -0.097613 -0.362521    1.000000\n"
     ]
    }
   ],
   "source": [
    "matrix = spruced_data.corr()\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b04b93d-99ba-4c7c-b8f6-8f0eafee353b",
   "metadata": {},
   "source": [
    "Single primer and prior_sim_val are highly correlated (to be expected)\n",
    "\n",
    "**Word length and the logorithmic word frequency are NOT (surprising!)**\n",
    "  * Could be a few reasons of this. For one, since these are texts made for kids, I'm guessing word frequency values would be quite high for most words. This means that irregardless of word length, word frequency could be similar. Though, I dont' think there's anything I can do to confirm this, other than maybe looking at a hsitogram to see the distribution of word frequency for words used in this anlaysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
